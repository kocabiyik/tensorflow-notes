{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    # Orwellâ€™s Six Rules for Writing\n",
    "    'Never use a metaphor, simile, or other figure of speech which you are used to seeing in print.',\n",
    "    'Never use a long word where a short one will do.',\n",
    "    'If it is possible to cut a word out, always cut it out.',\n",
    "    'Never use the passive where you can use the active.',\n",
    "    'Never use a foreign phrase, a scientific word, or a jargon word if you can think of an everyday English equivalent.',\n",
    "    'Break any of these rules sooner than say anything outright barbarous.',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=100, oov_token='<UNK>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<UNK>': 1,\n",
       " 'a': 2,\n",
       " 'use': 3,\n",
       " 'never': 4,\n",
       " 'word': 5,\n",
       " 'of': 6,\n",
       " 'you': 7,\n",
       " 'or': 8,\n",
       " 'to': 9,\n",
       " 'where': 10,\n",
       " 'if': 11,\n",
       " 'it': 12,\n",
       " 'cut': 13,\n",
       " 'out': 14,\n",
       " 'the': 15,\n",
       " 'can': 16,\n",
       " 'metaphor': 17,\n",
       " 'simile': 18,\n",
       " 'other': 19,\n",
       " 'figure': 20,\n",
       " 'speech': 21,\n",
       " 'which': 22,\n",
       " 'are': 23,\n",
       " 'used': 24,\n",
       " 'seeing': 25,\n",
       " 'in': 26,\n",
       " 'print': 27,\n",
       " 'long': 28,\n",
       " 'short': 29,\n",
       " 'one': 30,\n",
       " 'will': 31,\n",
       " 'do': 32,\n",
       " 'is': 33,\n",
       " 'possible': 34,\n",
       " 'always': 35,\n",
       " 'passive': 36,\n",
       " 'active': 37,\n",
       " 'foreign': 38,\n",
       " 'phrase': 39,\n",
       " 'scientific': 40,\n",
       " 'jargon': 41,\n",
       " 'think': 42,\n",
       " 'an': 43,\n",
       " 'everyday': 44,\n",
       " 'english': 45,\n",
       " 'equivalent': 46,\n",
       " 'break': 47,\n",
       " 'any': 48,\n",
       " 'these': 49,\n",
       " 'rules': 50,\n",
       " 'sooner': 51,\n",
       " 'than': 52,\n",
       " 'say': 53,\n",
       " 'anything': 54,\n",
       " 'outright': 55,\n",
       " 'barbarous': 56}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Is this a corpus? What is a corpus?\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('never', 4),\n",
       "             ('use', 5),\n",
       "             ('a', 7),\n",
       "             ('metaphor', 1),\n",
       "             ('simile', 1),\n",
       "             ('or', 2),\n",
       "             ('other', 1),\n",
       "             ('figure', 1),\n",
       "             ('of', 3),\n",
       "             ('speech', 1),\n",
       "             ('which', 1),\n",
       "             ('you', 3),\n",
       "             ('are', 1),\n",
       "             ('used', 1),\n",
       "             ('to', 2),\n",
       "             ('seeing', 1),\n",
       "             ('in', 1),\n",
       "             ('print', 1),\n",
       "             ('long', 1),\n",
       "             ('word', 4),\n",
       "             ('where', 2),\n",
       "             ('short', 1),\n",
       "             ('one', 1),\n",
       "             ('will', 1),\n",
       "             ('do', 1),\n",
       "             ('if', 2),\n",
       "             ('it', 2),\n",
       "             ('is', 1),\n",
       "             ('possible', 1),\n",
       "             ('cut', 2),\n",
       "             ('out', 2),\n",
       "             ('always', 1),\n",
       "             ('the', 2),\n",
       "             ('passive', 1),\n",
       "             ('can', 2),\n",
       "             ('active', 1),\n",
       "             ('foreign', 1),\n",
       "             ('phrase', 1),\n",
       "             ('scientific', 1),\n",
       "             ('jargon', 1),\n",
       "             ('think', 1),\n",
       "             ('an', 1),\n",
       "             ('everyday', 1),\n",
       "             ('english', 1),\n",
       "             ('equivalent', 1),\n",
       "             ('break', 1),\n",
       "             ('any', 1),\n",
       "             ('these', 1),\n",
       "             ('rules', 1),\n",
       "             ('sooner', 1),\n",
       "             ('than', 1),\n",
       "             ('say', 1),\n",
       "             ('anything', 1),\n",
       "             ('outright', 1),\n",
       "             ('barbarous', 1)])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word to sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 3, 2, 17, 18, 8, 19, 20, 6, 21, 22, 7, 23, 24, 9, 25, 26, 27],\n",
       " [4, 3, 2, 28, 5, 10, 2, 29, 30, 31, 32],\n",
       " [11, 12, 33, 34, 9, 13, 2, 5, 14, 35, 13, 12, 14],\n",
       " [4, 3, 15, 36, 10, 7, 16, 3, 15, 37],\n",
       " [4, 3, 2, 38, 39, 2, 40, 5, 8, 2, 41, 5, 11, 7, 16, 42, 6, 43, 44, 45, 46],\n",
       " [47, 48, 6, 49, 50, 51, 52, 53, 54, 55, 56]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = tokenizer.texts_to_sequences(sentences)\n",
    "seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4,  3,  2, 17, 18,  8, 19, 20,  6, 21, 22,  7, 23, 24,  9, 25,\n",
       "        26, 27,  0,  0,  0],\n",
       "       [ 4,  3,  2, 28,  5, 10,  2, 29, 30, 31, 32,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0],\n",
       "       [11, 12, 33, 34,  9, 13,  2,  5, 14, 35, 13, 12, 14,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0],\n",
       "       [ 4,  3, 15, 36, 10,  7, 16,  3, 15, 37,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0],\n",
       "       [ 4,  3,  2, 38, 39,  2, 40,  5,  8,  2, 41,  5, 11,  7, 16, 42,\n",
       "         6, 43, 44, 45, 46],\n",
       "       [47, 48,  6, 49, 50, 51, 52, 53, 54, 55, 56,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0]], dtype=int32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_sequences(seq, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
